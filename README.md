# Beyond Words: Integrating Image-Based Context for Situation-Grounded Procedural Planning

This project explores using step-wise, verifier-guided beam search for **vision-language procedural planning** in embodied AI settings.  
We extend prior text-only planning methods by conditioning plans not just on textual goals, but also on **egocentric visual context**.

Building off from [PlaSma](https://arxiv.org/abs/2305.19472) (symbolic verifier-guided planning), we develop a **stepwise beam search** system that iteratively generates action steps while consulting a vision-language verifier at each step.

Our work aims to answer three research questions:
- Does step-wise verifier-guided beam search improve planning compared to full-plan generation?
- How does the number of beams and candidates impact the quality of planning?
- Does finetuning on egocentric chain-of-thought ([EgoCOT](https://github.com/EmbodiedGPT/EgoCOT_Dataset)) data improve planning quality over few-shot prompting?

[Read our Paper (PDF)](Beyond_Words.pdf)

## ğŸ§  Key Contributions

- **Stepwise Planning with Verifier-Guided Beam Search**  
  Incrementally generate plans step-by-step, scoring candidate steps with a vision-language model-based verifier.

- **EgoCOT Finetuning**  
  Finetuned Qwen2.5-VL-3B on EgoCOT for next-step prediction in egocentric environments.

- **LLM-Based Evaluation**  
  Used GPT-4-turbo as a judge for pairwise plan comparisons based on conciseness, minimality, and actionability.

## ğŸ“‚ Project Structure

```plaintext
/
â”œâ”€â”€ Beyond_Words.ipynb          # (Main notebook: beam search, evaluation experiments, inference)
â”œâ”€â”€ Beyond_Words.pdf            # (Our research paper)
â”œâ”€â”€ README.md                   # (You're reading it!)
â”œâ”€â”€ requirements.txt            # (Python dependencies)
â”œâ”€â”€ checkpoints/                
â”‚   â”œâ”€â”€ qwen-finetuned-lora/     # (LoRA adapters for the finetuned planning model)
â”‚   â””â”€â”€ regressor.pt             # (Weights for the Qwen-based verifier model)
â”œâ”€â”€ data/                       
â”‚   â”œâ”€â”€ images_png/              # (Egocentric images for testing)
â”‚   â”œâ”€â”€ test_set.json            # (List of goals and associated image paths for evaluation)
â”‚   â””â”€â”€ verifier_data.json       # (Training schema for Qwen-based verifier model)
â”œâ”€â”€ planning_vlm/
â”‚   â”œâ”€â”€ train_logs/              # (Training logs for planner finetuning)
â”‚   â””â”€â”€ train_planner.py         # (Finetuning script for planner model)
â”œâ”€â”€ verifier_vlm/
â”‚   â”œâ”€â”€ dataset.py               # (Creates the verifier dataset structure)
â”‚   â”œâ”€â”€ model.py                 # (Regression model for computing continuous [0, 1] score)
â”‚   â”œâ”€â”€ training.py              # (Training script for verifier LoRA finetuning)
â”‚   â”œâ”€â”€ qwen_inference.py        # (Inference script using Qwen as verifier)
â”‚   â””â”€â”€ gpt_inference.py         # (Inference script using GPT API as verifier)
```

## ğŸš€ How to Run
1. **Install Dependencies**

    ```bash
    pip install -r requirements.txt
    ```

2. **Run Inference and Evaluation**

    The main experiments and results are inside `Beyond_Words.ipynb`.
   
    Open the notebook and run all cells sequentially to:
    
    - Perform verifier-guided beam search planning
    - Compare full-plan generation vs step-wise generation
    - Evaluate plans using GPT-4-turbo as a judge
    - Analyze results for all three research questions

4. **Finetune the Planner (Optional)**

    If you want to retrain the planning model (`Qwen2.5-VL-3B-Instruct`):

    ```bash
    python planning_vlm/train_planner.py
    ```

## ğŸ“ˆ Notes

- **Test Set**: We evaluate performance of our framework with 50 examples sampled from EgoCOT (located in `data/test_set.json`).

- **Verifier**: We switched from a trained verifier model (`verifier_vlm/`) to a GPT-4-turbo API-based verifier for better, more reliable scoring.

- **Evaluation Constraints**: We restricted to a maximum of 5 steps per plan, with optional early stopping using `[EOP]`.

## âš¡ Acknowledgements

- Our work is an extension of [**PlaSma: Symbolic Planning with Verifier-Guided Beam Search**](https://arxiv.org/abs/2305.19472).
- We use models and utilities from **Hugging Face**, **Qwen-VL**, and **OpenAI**.

## ğŸ† Contributors

- Vatsal Joshi, M.S. University of Michigan ([link](https://github.com/jvatsal21))
- Omkar Yadav, M.S. University of Michigan ([link](https://github.com/omkar-yadav-12))

